{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess = \"data\"\n",
    "ravdess_directory_list = os.listdir(ravdess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First and last few rows of the DataFrame:\n",
      "       Emotion                               File_Path\n",
      "0      neutral  data\\Actor_01\\03-01-01-01-01-01-01.wav\n",
      "1      neutral  data\\Actor_01\\03-01-01-01-01-02-01.wav\n",
      "2      neutral  data\\Actor_01\\03-01-01-01-02-01-01.wav\n",
      "3      neutral  data\\Actor_01\\03-01-01-01-02-02-01.wav\n",
      "4         calm  data\\Actor_01\\03-01-02-01-01-01-01.wav\n",
      "1435  surprise  data\\Actor_24\\03-01-08-01-02-02-24.wav\n",
      "1436  surprise  data\\Actor_24\\03-01-08-02-01-01-24.wav\n",
      "1437  surprise  data\\Actor_24\\03-01-08-02-01-02-24.wav\n",
      "1438  surprise  data\\Actor_24\\03-01-08-02-02-01-24.wav\n",
      "1439  surprise  data\\Actor_24\\03-01-08-02-02-02-24.wav\n"
     ]
    }
   ],
   "source": [
    "emotions = []\n",
    "paths = []\n",
    "\n",
    "for actor in ravdess_directory_list:\n",
    "    actor_path = os.path.join(ravdess, actor)\n",
    "    actor_files = os.listdir(actor_path)\n",
    "\n",
    "    for audio_file in actor_files:\n",
    "        file_parts = audio_file.split('.')[0].split('-')\n",
    "        if len(file_parts) > 2:\n",
    "            emotion_code = int(file_parts[2])\n",
    "            emotions.append(emotion_code)\n",
    "            paths.append(os.path.join(actor_path, audio_file))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Emotion': emotions,\n",
    "    'File_Path': paths\n",
    "})\n",
    "\n",
    "emotion_labels = {\n",
    "    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n",
    "    5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'\n",
    "}\n",
    "\n",
    "data['Emotion'] = data['Emotion'].map(emotion_labels)\n",
    "\n",
    "head_and_tail = pd.concat([data.head(), data.tail()])\n",
    "print(\"First and last few rows of the DataFrame:\")\n",
    "print(head_and_tail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, frame_length=2048, hop_length=512):\n",
    "        self.frame_length = frame_length\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def zcr(self, data):\n",
    "        return librosa.feature.zero_crossing_rate(data, frame_length=self.frame_length, hop_length=self.hop_length).flatten()\n",
    "\n",
    "    def rmse(self, data):\n",
    "        return librosa.feature.rms(y=data, frame_length=self.frame_length, hop_length=self.hop_length).flatten()\n",
    "\n",
    "    def mfcc(self, data, sr, n_mfcc=13, flatten=True):\n",
    "        mfcc_features = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=n_mfcc, hop_length=self.hop_length)\n",
    "        return mfcc_features.T.flatten() if flatten else mfcc_features.T\n",
    "\n",
    "    def chroma(self, data, sr):\n",
    "        chroma_features = librosa.feature.chroma_stft(y=data, sr=sr, hop_length=self.hop_length)\n",
    "        return chroma_features.T.flatten()\n",
    "\n",
    "    def spectral_contrast(self, data, sr):\n",
    "        contrast_features = librosa.feature.spectral_contrast(y=data, sr=sr, hop_length=self.hop_length)\n",
    "        return contrast_features.T.flatten()\n",
    "\n",
    "    def mel_spectrogram(self, data, sr):\n",
    "        mel_features = librosa.feature.melspectrogram(y=data, sr=sr, hop_length=self.hop_length)\n",
    "        return librosa.power_to_db(mel_features).flatten()\n",
    "\n",
    "    def extract_features(self, data, sr):\n",
    "        zcr_features = self.zcr(data)\n",
    "        rmse_features = self.rmse(data)\n",
    "        mfcc_features = self.mfcc(data, sr)\n",
    "        chroma_features = self.chroma(data, sr)\n",
    "        spectral_contrast_features = self.spectral_contrast(data, sr)\n",
    "        mel_spectrogram_features = self.mel_spectrogram(data, sr)\n",
    "        return np.concatenate([zcr_features,\n",
    "                                rmse_features,\n",
    "                                mfcc_features,\n",
    "                                chroma_features,\n",
    "                                spectral_contrast_features,\n",
    "                                mel_spectrogram_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    @staticmethod\n",
    "    def noise(data, noise_factor=0.005):\n",
    "        noise_amp = noise_factor * np.random.uniform() * np.amax(data)\n",
    "        return data + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    @staticmethod\n",
    "    def pitch(data, sr, n_steps=4):\n",
    "        return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, frame_length=2048, hop_length=512):\n",
    "        self.feature_extractor = FeatureExtractor(frame_length, hop_length)\n",
    "        self.augmenter = DataAugmentation()\n",
    "\n",
    "    def get_features(self, path, duration=2.5, offset=0.6):\n",
    "        data, sr = librosa.load(path, duration=duration, offset=offset)\n",
    "        features = [self.feature_extractor.extract_features(data, sr)]\n",
    "\n",
    "        noised_audio = self.augmenter.noise(data)\n",
    "        features.append(self.feature_extractor.extract_features(noised_audio, sr))\n",
    "\n",
    "        pitched_audio = self.augmenter.pitch(data, sr)\n",
    "        features.append(self.feature_extractor.extract_features(pitched_audio, sr))\n",
    "\n",
    "        pitched_noised_audio = self.augmenter.noise(pitched_audio)\n",
    "        features.append(self.feature_extractor.extract_features(pitched_noised_audio, sr))\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    def process_feature(self, path, emotion):\n",
    "        features = self.get_features(path)\n",
    "        X = features.tolist()\n",
    "        Y = [emotion] * len(features)\n",
    "        return X, Y\n",
    "\n",
    "    def process_dataset(self, df, n_jobs=-1):\n",
    "        paths = df['File_Path'].values\n",
    "        emotions = df['Emotion'].values\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(self.process_feature)(path, emotion) for path, emotion in zip(paths, emotions))\n",
    "\n",
    "        X, Y = [], []\n",
    "        for result in results:\n",
    "            X.extend(result[0])\n",
    "            Y.extend(result[1])\n",
    "\n",
    "        max_len = max(len(x) for x in X)\n",
    "        X = np.array([np.pad(x, (0, max_len - len(x)), 'constant') if len(x) < max_len else x[:max_len] for x in X])\n",
    "\n",
    "        return X, np.array(Y)\n",
    "\n",
    "# Process the dataset\n",
    "processor = AudioProcessor()\n",
    "X, Y = processor.process_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4608, 17496, 1, 1) (4608, 8) (1152, 17496, 1, 1) (1152, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "def prepare_data_for_cnn(X, Y):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    Y = encoder.fit_transform(Y)\n",
    "    joblib.dump(encoder, 'label_encoder.pkl')\n",
    "\n",
    "    num_classes = len(np.unique(Y))\n",
    "    Y = to_categorical(Y, num_classes=num_classes)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test, num_classes\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, num_classes = prepare_data_for_cnn(X, Y)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sweta\\Desktop\\Tasks 1\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 254ms/step - accuracy: 0.3248 - loss: 1.9655 - val_accuracy: 0.5408 - val_loss: 1.2076\n",
      "Epoch 2/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 234ms/step - accuracy: 0.7215 - loss: 0.8201 - val_accuracy: 0.7995 - val_loss: 0.5502\n",
      "Epoch 3/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 250ms/step - accuracy: 0.9530 - loss: 0.2123 - val_accuracy: 0.8976 - val_loss: 0.3410\n",
      "Epoch 4/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 248ms/step - accuracy: 0.9897 - loss: 0.0608 - val_accuracy: 0.9149 - val_loss: 0.2961\n",
      "Epoch 5/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 230ms/step - accuracy: 0.9995 - loss: 0.0121 - val_accuracy: 0.9366 - val_loss: 0.2402\n",
      "Epoch 6/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 226ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.9375 - val_loss: 0.2355\n",
      "Epoch 7/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 224ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9384 - val_loss: 0.2468\n",
      "Epoch 8/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 389ms/step - accuracy: 1.0000 - loss: 8.8032e-04 - val_accuracy: 0.9410 - val_loss: 0.2577\n",
      "Epoch 9/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 364ms/step - accuracy: 1.0000 - loss: 6.5628e-04 - val_accuracy: 0.9401 - val_loss: 0.2605\n",
      "Epoch 10/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 349ms/step - accuracy: 1.0000 - loss: 4.9275e-04 - val_accuracy: 0.9384 - val_loss: 0.2610\n",
      "Epoch 11/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 370ms/step - accuracy: 1.0000 - loss: 3.8896e-04 - val_accuracy: 0.9401 - val_loss: 0.2691\n",
      "Epoch 12/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 366ms/step - accuracy: 1.0000 - loss: 3.3404e-04 - val_accuracy: 0.9401 - val_loss: 0.2704\n",
      "Epoch 13/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 2.6887e-04 - val_accuracy: 0.9366 - val_loss: 0.2706\n",
      "Epoch 14/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 231ms/step - accuracy: 1.0000 - loss: 2.2914e-04 - val_accuracy: 0.9375 - val_loss: 0.2751\n",
      "Epoch 15/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 242ms/step - accuracy: 1.0000 - loss: 1.9115e-04 - val_accuracy: 0.9392 - val_loss: 0.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(MaxPooling2D((2, 1)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "cnn_model = build_cnn_model(X_train.shape[1:], num_classes)\n",
    "cnn_model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data=(X_test, Y_test))\n",
    "cnn_model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.9347 - loss: 0.3368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27826908230781555, 0.9392361044883728]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001EC3A5F1D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001EC3A5F1D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the trained model, scaler, and label encoder\n",
    "model = load_model('cnn_model.h5')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Ensure AudioProcessor is defined or imported properly\n",
    "processor = AudioProcessor()  # Make sure this class or function exists\n",
    "\n",
    "def record_audio():\n",
    "    fs = 44100  # Sample rate\n",
    "    duration = 5  # Duration in seconds\n",
    "    print(\"Recording...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished\")\n",
    "    # Save the audio file\n",
    "    audio = audio.flatten()\n",
    "    sf.write('recorded_audio.wav', audio, fs)  # Use soundfile instead of librosa\n",
    "    return 'recorded_audio.wav'\n",
    "\n",
    "def open_file():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Audio Files\", \"*.wav *.mp3\")])\n",
    "    return file_path\n",
    "\n",
    "def predict_emotion(file_path):\n",
    "    features = processor.get_features(file_path)\n",
    "    features = scaler.transform([features])\n",
    "    features = features.reshape((features.shape[0], features.shape[1], 1, 1))\n",
    "    prediction = model.predict(features)\n",
    "    predicted_emotion = label_encoder.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
    "    return predicted_emotion\n",
    "\n",
    "def record_and_predict():\n",
    "    try:\n",
    "        file_path = record_audio()\n",
    "        emotion = predict_emotion(file_path)\n",
    "        messagebox.showinfo(\"Prediction\", f\"Predicted Emotion: {emotion}\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def select_and_predict():\n",
    "    try:\n",
    "        file_path = open_file()\n",
    "        emotion = predict_emotion(file_path)\n",
    "        messagebox.showinfo(\"Prediction\", f\"Predicted Emotion: {emotion}\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Set up the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Emotion Recognition\")\n",
    "root.geometry(\"400x200\")  # Set the window size to 400x200 pixels\n",
    "\n",
    "record_button = tk.Button(root, text=\"Record Audio\", command=record_and_predict)\n",
    "record_button.pack(pady=10)\n",
    "\n",
    "select_button = tk.Button(root, text=\"Select Audio File\", command=select_and_predict)\n",
    "select_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 107620,
     "sourceId": 256618,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
